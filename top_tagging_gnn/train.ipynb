{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from model import ParticleNet\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "from utils import load_data, make_roc, save_model\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# Ignore divide by 0 errors\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "\n",
    "# define the global base device\n",
    "world_size = torch.cuda.device_count()\n",
    "multi_gpu = world_size >= 2\n",
    "if world_size:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    for i in range(world_size):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Device: CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation_run(multi_gpu, device, model, loader):\n",
    "    with torch.no_grad():\n",
    "        optimizer = None\n",
    "        ret = train(\n",
    "            multi_gpu,\n",
    "            device,\n",
    "            model,\n",
    "            loader,\n",
    "            optimizer,\n",
    "        )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(multi_gpu, device, model, loader, optimizer):\n",
    "    \"\"\"\n",
    "    A training/validation run over a given epoch that gets called in the training_loop() function.\n",
    "    When optimizer is set to None, it freezes the model for a validation_run.\n",
    "    \"\"\"\n",
    "\n",
    "    is_train = not (optimizer is None)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    sig = nn.Sigmoid()\n",
    "\n",
    "    # initialize loss and time counters\n",
    "    losses, t = 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        if multi_gpu:\n",
    "            batch = batch\n",
    "        else:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        # run forward pass\n",
    "        t0 = time.time()\n",
    "        preds, targets = model(batch)\n",
    "        t1 = time.time()\n",
    "        t += t1 - t0\n",
    "\n",
    "        loss = criterion(sig(preds), targets.reshape(-1, 1).float())\n",
    "\n",
    "        # backprop\n",
    "        if is_train:  # not run during a validation run\n",
    "            for param in model.parameters():\n",
    "                # better than calling optimizer.zero_grad()\n",
    "                # according to https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "                param.grad = None\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses = losses + loss.detach()\n",
    "\n",
    "    print(f\"Average inference time per batch is {round((t / len(loader)), 3)}s\")\n",
    "\n",
    "    losses = (losses / (len(loader))).cpu().item()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    multi_gpu,\n",
    "    device,\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    patience,\n",
    "    optimizer,\n",
    "    outpath,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to perform training. Will call the train() and validation_run() functions every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: a pytorch model wrapped by DistributedDataParallel (DDP)\n",
    "        train_loader: a pytorch Dataloader that loads .pt files for training when you invoke the get() method\n",
    "        valid_loader: a pytorch Dataloader that loads .pt files for validation when you invoke the get() method\n",
    "        patience: number of stale epochs allowed before stopping the training\n",
    "        optimizer: optimizer to use for training (by default: Adam)\n",
    "        outpath: path to store the model weights and training plots\n",
    "    \"\"\"\n",
    "\n",
    "    # create directory to hold loss plots\n",
    "    if not os.path.exists(f\"{outpath}/loss_plots/\"):\n",
    "        os.makedirs(f\"{outpath}/loss_plots/\")\n",
    "\n",
    "    # create directory to hold the model state at each epoch\n",
    "    if not os.path.exists(f\"{outpath}/epoch_weights/\"):\n",
    "        os.makedirs(f\"{outpath}/epoch_weights/\")\n",
    "\n",
    "    t0_initial = time.time()\n",
    "\n",
    "    losses_train, losses_valid = [], []\n",
    "\n",
    "    best_val_loss = 99999.9\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        t0 = time.time()\n",
    "\n",
    "        if stale_epochs > patience:\n",
    "            print(\"breaking due to stale epochs\")\n",
    "            break\n",
    "\n",
    "        # training step\n",
    "        print(\"---->Initiating a training run\")\n",
    "        model.train()\n",
    "        losses = train(\n",
    "            multi_gpu,\n",
    "            device,\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "        )\n",
    "\n",
    "        losses_train.append(losses)\n",
    "\n",
    "        # validation step\n",
    "        print(\"---->Initiating a validation run\")\n",
    "        model.eval()\n",
    "        losses = validation_run(multi_gpu, device, model, valid_loader)\n",
    "\n",
    "        losses_valid.append(losses)\n",
    "\n",
    "        # early-stopping\n",
    "        if losses < best_val_loss:\n",
    "            best_val_loss = losses\n",
    "            stale_epochs = 0\n",
    "\n",
    "            try:\n",
    "                state_dict = model.module.state_dict()\n",
    "            except AttributeError:\n",
    "                state_dict = model.state_dict()\n",
    "            torch.save(state_dict, f\"{outpath}/best_epoch_weights.pth\")\n",
    "\n",
    "            with open(f\"{outpath}/best_epoch.json\", \"w\") as fp:  # dump best epoch\n",
    "                json.dump({\"best_epoch\": epoch}, fp)\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        epochs_remaining = n_epochs - (epoch + 1)\n",
    "        time_per_epoch = (t1 - t0_initial) / (epoch + 1)\n",
    "        eta = epochs_remaining * time_per_epoch / 60\n",
    "\n",
    "        print(\n",
    "            f\"epoch={epoch + 1} / {n_epochs} \"\n",
    "            + f\"train_loss={round(losses_train[epoch], 4)} \"\n",
    "            + f\"valid_loss={round(losses_valid[epoch], 4)} \"\n",
    "            + f\"stale={stale_epochs} \"\n",
    "            + f\"time={round((t1-t0)/60, 2)}m \"\n",
    "            + f\"eta={round(eta, 1)}m\"\n",
    "        )\n",
    "\n",
    "        # save the model's weights\n",
    "        try:\n",
    "            state_dict = model.module.state_dict()\n",
    "        except AttributeError:\n",
    "            state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f\"{outpath}/epoch_weights/epoch_{epoch+1}_weights.pth\")\n",
    "\n",
    "        # make loss plots\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(losses_train)), losses_train, label=\"training\")\n",
    "        ax.plot(range(len(losses_valid)), losses_valid, label=\"validation\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        plt.savefig(f\"{outpath}/loss_plots/losses_epoch_{epoch}.pdf\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        with open(f\"{outpath}/loss_plots/losses_epoch_{epoch}.pkl\", \"wb\") as f:\n",
    "            pkl.dump((losses_train, losses_valid), f)\n",
    "\n",
    "        print(\"----------------------------------------------------------\")\n",
    "    print(f\"Done with training. Total training time is {round((time.time() - t0_initial)/60,3)}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "dataset = \"data/toptagging/train\"\n",
    "# directory to hold the model and plots\n",
    "model_prefix = \"ParticleNet_model\"\n",
    "outpath = osp.join(\"experiments/\", model_prefix)\n",
    "# number of epochs to train\n",
    "n_epochs = 3\n",
    "# batch size\n",
    "batch_size = 128\n",
    "# patience before early stopping\n",
    "patience = 20\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "# k nearest neighbors in gravnet layer\n",
    "nearest = 16\n",
    "# depth of DNN in each EdgeConv block\n",
    "depth = 1\n",
    "# add dropout\n",
    "dropout = False\n",
    "# perform quick training and testing\n",
    "quick = False\n",
    "# whether to overwrite the model\n",
    "overwrite = False\n",
    "# setup the input/output dimension of the model\n",
    "num_features = 7\n",
    "num_classes = 1\n",
    "\n",
    "outpath = osp.join(\"experiments/\", model_prefix)\n",
    "\n",
    "if not osp.isdir(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "model_kwargs = {\n",
    "    \"for_LRP\": False,\n",
    "    \"node_feat_size\": num_features,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"k\": nearest,\n",
    "    \"depth\": depth,\n",
    "    \"dropout\": dropout\n",
    "}\n",
    "\n",
    "model = ParticleNet(**model_kwargs)\n",
    "\n",
    "print(model)\n",
    "print(f\"Model prefix: {model_prefix}\")\n",
    "\n",
    "# save model_kwargs and hyperparameters\n",
    "save_model(\n",
    "    outpath,\n",
    "    model_kwargs,\n",
    "    model.kernel_sizes,\n",
    "    model.fc_size,\n",
    "    model.dropout,\n",
    "    depth,\n",
    "    overwrite,\n",
    "    n_epochs,\n",
    ")\n",
    "\n",
    "# save the weights before training for lrp comparisons\n",
    "try:\n",
    "    state_dict = model.module.state_dict()\n",
    "except AttributeError:\n",
    "    state_dict = model.state_dict()\n",
    "torch.save(state_dict, f\"{outpath}/before_training_weights.pth\")\n",
    "\n",
    "# Load the training datafiles\n",
    "print(\"- loading datafiles for training...\")\n",
    "data_train = load_data(dataset, \"train\", 12, quick)\n",
    "data_valid = load_data(dataset, \"val\", 4, quick)\n",
    "\n",
    "# make convenient dataloaders and use DataParallel if multi_gpu is on\n",
    "if multi_gpu:\n",
    "    train_loader = DataListLoader(data_train, batch_size=batch_size)\n",
    "    valid_loader = DataListLoader(data_valid, batch_size=batch_size)\n",
    "    model = torch_geometric.nn.DataParallel(model)\n",
    "else:\n",
    "    train_loader = DataLoader(data_train, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(data_valid, batch_size=batch_size)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"- training over {n_epochs} epochs\")\n",
    "training_loop(\n",
    "    multi_gpu,\n",
    "    device,\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    patience,\n",
    "    optimizer,\n",
    "    outpath,\n",
    ")\n",
    "\n",
    "# load the best trained model for testing\n",
    "with open(f\"{outpath}/model_kwargs.pkl\", \"rb\") as f:\n",
    "    model_kwargs = pkl.load(f)\n",
    "\n",
    "state_dict = torch.load(f\"{outpath}/best_epoch_weights.pth\", map_location=device)\n",
    "\n",
    "model = ParticleNet(**model_kwargs)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"- loading datafiles for testing...\")\n",
    "data_test = load_data(dataset, \"test\", 4, quick)\n",
    "\n",
    "if multi_gpu:\n",
    "    test_loader = DataListLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "    model = torch_geometric.nn.DataParallel(model)\n",
    "else:\n",
    "    test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"- making predictions\")\n",
    "y_score = None\n",
    "y_test = None\n",
    "for i, batch in enumerate(test_loader):\n",
    "    if multi_gpu:\n",
    "        batch = batch\n",
    "    else:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "    preds, targets = model(batch)\n",
    "    preds = preds.detach().cpu()\n",
    "\n",
    "    if y_score is None:\n",
    "        y_score = preds[:].detach().cpu().reshape(-1)\n",
    "        y_test = targets.detach().cpu()\n",
    "    else:\n",
    "        y_score = torch.cat([y_score, preds[:].detach().cpu().reshape(-1)])\n",
    "        y_test = torch.cat([y_test, targets.detach().cpu()])\n",
    "\n",
    "# save the predictions\n",
    "print(\"- saving predictions\")\n",
    "torch.save(y_test, f\"{outpath}/y_test.pt\")\n",
    "torch.save(y_score, f\"{outpath}/y_score.pt\")\n",
    "\n",
    "# Compute ROC curve\n",
    "print(\"- making Roc curves\")\n",
    "make_roc(y_test, y_score, f\"{outpath}/Roc_curve.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
