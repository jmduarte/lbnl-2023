{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle as pkl\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from lrp_particlenet import LRP_ParticleNet\n",
    "from model import ParticleNet\n",
    "from utils import load_data, make_dr_Mij_plots\n",
    "from fastjet_utils import scaling_up\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if the GPU configuration and define the global base device\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"Will use GPU model:\", torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Will use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script runs lrp on a trained ParticleNet model\n",
    "dataset = \"data/toptagging\"\n",
    "outpath = \"experiments\"\n",
    "model_prefix = \"ParticleNet_dropout1\"\n",
    "epoch = -1  # epoch to run Rscores for (-1=best_epoch, 0=untrained)\n",
    "quick = False\n",
    "run_lrp = True\n",
    "run_dr_Mij_plots = True\n",
    "run_scaling_up = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = osp.join(outpath, model_prefix)\n",
    "\n",
    "if not os.path.exists(f\"{outpath}/xai\"):\n",
    "    os.makedirs(f\"{outpath}/xai\")\n",
    "\n",
    "# run lrp pipeline to compute the Rscores\n",
    "if run_lrp:\n",
    "    print(f\"Runing the LRP pipeline to compute the Rscores for the model at epoch {epoch}\")\n",
    "    # load the testing data\n",
    "    print(\"- loading datafiles for lrp studies...\")\n",
    "    data_test = load_data(dataset, \"test\", 4, quick)\n",
    "    loader = DataLoader(data_test, batch_size=1, shuffle=True)\n",
    "\n",
    "    # load a pretrained model\n",
    "    with open(f\"{outpath}/model_kwpkl\", \"rb\") as f:\n",
    "        model_kwargs = pkl.load(f)\n",
    "\n",
    "    if epoch == -1:  # load the best trained model\n",
    "        state_dict = torch.load(f\"{outpath}/best_epoch_weights.pth\", map_location=device)\n",
    "        PATH = f\"{outpath}/xai/Rscores_best/\"\n",
    "    elif epoch == 0:  # load the untrained model\n",
    "        state_dict = torch.load(\n",
    "            f\"{outpath}/before_training_weights.pth\",\n",
    "            map_location=device,\n",
    "        )\n",
    "        PATH = f\"{outpath}/xai/Rscores_untrained/\"\n",
    "    else:  # load a specefic epoch of the trained model\n",
    "        state_dict = torch.load(\n",
    "            f\"{outpath}/epoch_weights/epoch_{epoch}_weights.pth\",\n",
    "            map_location=device,\n",
    "        )\n",
    "        PATH = f\"{outpath}/xai/Rscores_epoch_{epoch}/\"\n",
    "\n",
    "    # the following line will make it possible to retrieve intermediate activations\n",
    "    model_kwargs[\"for_LRP\"] = True\n",
    "\n",
    "    # instantiate a ParticleNet model with the loaded configuration\n",
    "    model = ParticleNet(**model_kwargs)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "\n",
    "    # make directory to hold the edgeRscores\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "\n",
    "    # initilize an lrp instance\n",
    "    lrp = LRP_ParticleNet(device=device, model=model, epsilon=1e-8)\n",
    "\n",
    "    # initilize placeholders to store the input, target, and p4 for each jet\n",
    "    batch_x_list, batch_y_list, batch_p4_list = [], [], []\n",
    "    # initilize placeholders to hold the edgeRscores and edge_index of each EdgeConv block for each jet\n",
    "    R_edges_list, edge_index_list = [], []\n",
    "\n",
    "    ti = time.time()\n",
    "    for i, jet in enumerate(loader):\n",
    "        if i == 5 and quick:\n",
    "            break\n",
    "\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "        print(f\"Explaining jet # {i}: {jet}\")\n",
    "\n",
    "        # explain a single jet\n",
    "        try:\n",
    "            R_edges, edge_index = lrp.explain(jet.to(device))\n",
    "        except Exception:\n",
    "            print(\"jet is not processed correctly so skipping it\")\n",
    "            continue\n",
    "\n",
    "        batch_x_list.append(jet.x.detach().cpu())\n",
    "        batch_y_list.append(jet.y.detach().cpu())\n",
    "\n",
    "        # for fast jet, store the p4 information\n",
    "        batch_p4_list.append(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    jet.px.detach().cpu().unsqueeze(1),\n",
    "                    jet.py.detach().cpu().unsqueeze(1),\n",
    "                    jet.pz.detach().cpu().unsqueeze(1),\n",
    "                    jet.E.detach().cpu().unsqueeze(1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        R_edges_list.append(R_edges)\n",
    "        edge_index_list.append(edge_index)\n",
    "\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "    tf = time.time()\n",
    "\n",
    "    with open(f\"{PATH}/time.json\", \"w\") as fp:  # dump time\n",
    "        json.dump(\n",
    "            {\n",
    "                \"time_min\": round((tf - ti) / 60, 3),\n",
    "                \"time_hours\": round((tf - ti) / 3600, 3),\n",
    "            },\n",
    "            fp,\n",
    "        )\n",
    "\n",
    "    # store the lists containing the jet information\n",
    "    with open(f\"{PATH}/batch_x.pkl\", \"wb\") as handle:\n",
    "        pkl.dump(batch_x_list, handle)\n",
    "    with open(f\"{PATH}/batch_y.pkl\", \"wb\") as handle:\n",
    "        pkl.dump(batch_y_list, handle)\n",
    "    with open(f\"{PATH}/batch_p4.pkl\", \"wb\") as handle:\n",
    "        pkl.dump(batch_p4_list, handle)\n",
    "\n",
    "    # store the lists containing the edgeRscores and edge_index of each EdgeConv block for each jet\n",
    "    with open(f\"{PATH}/R_edges.pkl\", \"wb\") as handle:\n",
    "        pkl.dump(R_edges_list, handle)\n",
    "    with open(f\"{PATH}/edge_index.pkl\", \"wb\") as handle:\n",
    "        pkl.dump(edge_index_list, handle)\n",
    "\n",
    "    print(f\"Finished computing the Rscores for the model at epoch {epoch}\")\n",
    "\n",
    "# produce the lrp deltaR and Mij result plots\n",
    "if run_dr_Mij_plots:\n",
    "    print(f\"Computing the deltaR and invariant mass distributions of the most relevant edges at epoch {epoch}\")\n",
    "    make_dr_Mij_plots(f\"{outpath}/xai\", epoch=epoch, Top_N=5)\n",
    "\n",
    "# produce the lrp scaling_up result plots\n",
    "if run_scaling_up:\n",
    "    print(f\"Computing the fraction of relevant edges connecting different subjets for the model at epoch {epoch}\")\n",
    "    scaling_up(\n",
    "        f\"{outpath}/xai\",\n",
    "        epoch=epoch,\n",
    "        N_values=15,\n",
    "        N_SUBJETS=3,\n",
    "        JET_ALGO=\"CA\",\n",
    "        jet_radius=0.8,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
