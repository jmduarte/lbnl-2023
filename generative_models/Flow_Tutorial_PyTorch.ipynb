{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-DStRWBSe_3"
   },
   "source": [
    "# Generative models exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This exercise is based on a normalizing flow exercise designed by T.Quadfasel, M.Sommerhalder and S.Diefenbacher, https://github.com/uhh-pd-ml/flow-exercise\n",
    "\n",
    "Broadly speaking the exercise is organized into three parts.\n",
    "- The first part takes a look at examples of some of the introduced generative models using the Two Moons data set\n",
    "- The second part focuses on normalizing flows and how to build them using the nFlows package\n",
    "- the third part then applies the flow methods to real physics data\n",
    "\n",
    "In order to run the exercise a special environment is needed. In a terminal run the following 3 lines:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "module load python\n",
    "\n",
    "source activate /global/cfs/cdirs/ntrain2/training_env_pytorch/\n",
    "\n",
    "python -m ipykernel install --user --name training_env_pytorch --display-name \"ATLAS Training (PyTorch)\"\n",
    "```\n",
    "Then swap the kernel of this notebook to 'ATLAS Training (PyTorch)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Moons Data Set\n",
    "\n",
    "One common example used for benchmarking is the so-called 'two moon' dataset, consisting of two interlocking half circles. On the one hand, it tests the generative model's ability to replicate a complex structure, on the other hand, this data set also features two separate subsets, which our generative model will have to keep separate as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJyPtwUJSfAH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we will get the dataset from scikit-learn\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for two moons dataset\n",
    "\n",
    "\n",
    "# define normalization function\n",
    "def normalize_moons(in_data):\n",
    "    max_val = np.max(in_data, keepdims=True, axis=0)\n",
    "    min_val = np.min(in_data, keepdims=True, axis=0)\n",
    "\n",
    "    new_data = (in_data - min_val) / (max_val - min_val)\n",
    "    mask = np.prod(((new_data < 1) & (new_data > 0)), axis=1, dtype=bool)\n",
    "    new_data = new_data[mask]\n",
    "    return new_data, mask\n",
    "\n",
    "\n",
    "n = 10000\n",
    "X_moons, _ = datasets.make_moons(n_samples=n, noise=0.05)\n",
    "X_moons, _ = normalize_moons(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.savefig(\"./original_two_moons.pdf\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "In our next step, we import the PyTorch package, which we will be using as our machine learning backend. We also implement a generic fully connected neural network class, which we will be using in the following generative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our usual PyTorch functionality\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# General purpose dense net\n",
    "class DenseNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, activation_fn=F.relu, last_activation_fn=None):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(self.input_dim, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.layers.append(torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "\n",
    "        self.layers.append(torch.nn.Linear(self.hidden_dim, self.output_dim))\n",
    "\n",
    "        self.activation_fn = activation_fn\n",
    "        self.last_activation_fn = last_activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = self.activation_fn(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        if self.last_activation_fn is not None:\n",
    "            x = self.last_activation_fn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.) Generative Models on Two Moons\n",
    "\n",
    "## 1a.) Generative Adversarial Network\n",
    "\n",
    "The first network we will investigate is the Generative Adversarial Network or GAN. As introduced in the lecture, the GAN consists of two separate networks, a Generator, and a Discriminator. GAN trainings consist of two steps, that are performed for each batch. \n",
    "\n",
    "In the first step, the Discriminator is trained to separate real samples from the dataset from fake samples from the generator. This is done using binary cross-entropy loss with synthetic labels that are defined such that real samples have the label `1` and fake samples have the label `0`. \n",
    "\n",
    "In the second step, the Generator is trained using the Discriminator output as its loss function. Note that to evaluate the Discriminator output we still use BCE, however, the labels are flipped, as the training objectives of the Generator and Discriminator are opposed to each other (or 'Adversarial')\n",
    "\n",
    "\n",
    "\n",
    "**Task: The GAN implementation below is functional, but far from optimal. Run the subsequent two cells, and observe the Two Moons samples produced by the generator. Then modify some of the marked parameters and observe if the result improves.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "### Modifiy network parameters here ###\n",
    "GeneratorNet = DenseNet(input_dim=8, output_dim=2, hidden_dim=16, n_layers=2).to(device)\n",
    "DiscriminatorNet = DenseNet(input_dim=2, output_dim=1, hidden_dim=16, n_layers=2, last_activation_fn=F.sigmoid).to(device)\n",
    "#######################################\n",
    "\n",
    "GeneratorOpt = optim.Adam(GeneratorNet.parameters(), lr=1e-4)\n",
    "DiscriminatorOpt = optim.Adam(DiscriminatorNet.parameters(), lr=1e-4)\n",
    "\n",
    "#######################################\n",
    "### modify training parameters here ###\n",
    "batch_size = 200\n",
    "epochs = 1\n",
    "#######################################\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "BCEloss = nn.BCELoss()\n",
    "for ep in range(epochs):\n",
    "    for i_batch in range(max_batches):\n",
    "        # select the current batch from the dataset\n",
    "        x_real = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x_real = torch.tensor(x_real, device=device).float()\n",
    "\n",
    "        DiscriminatorOpt.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn((batch_size, 8), device=device).float()\n",
    "            x_fake = GeneratorNet(noise)\n",
    "\n",
    "        y_real = torch.ones(batch_size, 1)\n",
    "        y_fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "        y = torch.cat((y_real, y_fake), 0)\n",
    "        x = torch.cat((x_real, x_fake), 0)\n",
    "\n",
    "        Discriminator_loss = BCEloss(DiscriminatorNet(x), y)\n",
    "        Discriminator_loss = Discriminator_loss.mean()\n",
    "        Discriminator_loss.backward()\n",
    "        DiscriminatorOpt.step()\n",
    "\n",
    "        GeneratorOpt.zero_grad()\n",
    "\n",
    "        noise = torch.randn((batch_size, 8), device=device).float()\n",
    "        x_fake = GeneratorNet(noise)\n",
    "\n",
    "        Generator_loss = BCEloss(DiscriminatorNet(x_fake), y_real)\n",
    "\n",
    "        Generator_loss = Generator_loss.mean()\n",
    "        Generator_loss.backward()\n",
    "        GeneratorOpt.step()\n",
    "    if ep % 100 == 0:\n",
    "        print(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use the fitted distribution and sample from it\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn((1000, 8), device=device).float()\n",
    "    samples = GeneratorNet(noise).cpu().numpy()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b.) Variational AutoEncoder\n",
    "\n",
    "The Variational Autoencoder implementation shown here explicitly defines the encoder and decoder models as two separate dense networks. The Endocer input and Decoder output have to match the dimensionality of the data set. The  Endocer output and Decoder input define the latent space their size is arbitrary, however, they need to be consistent such that the Encoder output is twice as large as the Decoder input. This is due to the fact in order to produce the Gaussian latent space, the Encoder needs to output a width and a mean for each Decoder input.\n",
    "\n",
    "In principle, this specific VAE is a Beta-VAE, as it has an explicit scaling parameter between the MSE reconstruction loss and the KLD latent regularization loss. However, for a beta parameter of 1, this setup is identical to a standard VAE. \n",
    "\n",
    "**Task: The GAN implementation below is functional, but far from optimal. Run the subsequent two cells, and observe the Two Moons samples produced by the generator. Then modify some of the marked parameters and observe if the result improves. Especially the Beta parameter is interesting to vary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Modifiy network parameters here ###\n",
    "EncoderNet = DenseNet(input_dim=2, output_dim=4, hidden_dim=32, n_layers=4).to(device)\n",
    "DecoderNet = DenseNet(input_dim=2, output_dim=2, hidden_dim=32, n_layers=4).to(device)\n",
    "#######################################\n",
    "\n",
    "\n",
    "EncoderOpt = optim.Adam(EncoderNet.parameters(), lr=1e-4)\n",
    "DecoderOpt = optim.Adam(DecoderNet.parameters(), lr=1e-4)\n",
    "\n",
    "#######################################\n",
    "### modify training parameters here ###\n",
    "batch_size = 200\n",
    "epochs = 1\n",
    "#######################################\n",
    "\n",
    "###################################\n",
    "### modify loss parameters here ###\n",
    "beta = 1\n",
    "###################################\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "MSEloss = nn.MSELoss()\n",
    "for ep in range(epochs):\n",
    "    for i_batch in range(max_batches):\n",
    "        EncoderOpt.zero_grad()\n",
    "        DecoderOpt.zero_grad()\n",
    "\n",
    "        # select the current batch from the dataset\n",
    "        x_real = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x_real = torch.tensor(x_real, device=device).float()\n",
    "\n",
    "        latent = EncoderNet(x_real)\n",
    "        mu = latent[:, ::2]\n",
    "        log_var = latent[:, 1::2]\n",
    "\n",
    "        KLD = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0)\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        reparameterized = eps * std + mu\n",
    "\n",
    "        x_recon = DecoderNet(reparameterized)\n",
    "\n",
    "        MSE = MSEloss(x_real, x_recon)\n",
    "\n",
    "        loss = KLD + MSE * beta\n",
    "        loss.backward()\n",
    "\n",
    "        EncoderOpt.step()\n",
    "        DecoderOpt.step()\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        print(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use the fitted distribution and sample from it\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn((1000, 2), device=device).float()\n",
    "    samples = DecoderNet(noise).cpu().numpy()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHlmT4cuSe_-"
   },
   "source": [
    "# 2 Normalizing Flows\n",
    "\n",
    "## 2a.) Motivation\n",
    "\n",
    "Just as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), Normalizing Flows are **Probabilistic Generative Models (PGMs)**, which describe a probability distribution that we attempt to learn from a set of observed data. \n",
    "\n",
    "PGMs are useful for generating new samples from the learned distribution, evaluating the likelihood of new data points, etc. \n",
    "\n",
    "Normalizing flows are PGMs built on **invertible transformations**. Their advantages are that it is typically possible to efficiently **sample** and **evaluate** the learned distributions. Normalizing flows also are **highly expressive** and come with a **useful latent space representation**, since we have a one-to-one mapping between our input and the latent space. Finally, they are also **easy to train**, since we just need to conduct a simple maximum likelihood training.\n",
    "\n",
    "In this hands-on exercise, we will show examples of using normalizing flows to estimate (conditional) probability densities and generate new samples. While some instructive mock datasets will be used for getting to know Normalizing Flows, we will also have one dedicated exercise that will use a Particle Physics dataset in the end. As a framework for actually implementing Normalizing Flows quickly, we will use the **nFlows** package. We hope this tutorial will give you at least a rough idea of how Normalizing Flows work in practice and - most importantly - that you find it instructive and fun :)\n",
    "\n",
    "## 2b.) Normalizing Flows - The general idea (recap)\n",
    "\n",
    "At the heart of normalizing flows stands the change of variables formula. Let's say we had a random variable $U$ and now, we apply a simple transformation, defining the transformed random variable as $X$. Then, the change of variable formula is given by:\n",
    "\n",
    "$$ p(x) = p(u)\\left| \\frac{df(u)}{du}\\right|^{-1} $$\n",
    "\n",
    "As an example, let's have a look at the figure below:\n",
    "\n",
    "<br />\n",
    "<img src=\"https://drive.google.com/uc?id=1SoaFiiAtpYMSOepjtZkgZACQeV4Zxwbr\" width=\"250\">\n",
    "<br />\n",
    "\n",
    "We start with a uniform distribution $U$ between 0 and 1, and then do a transformation $f(U)=2\\cdot U + 1$. This, however, leads to a distribution that is not normalized, so to get a proper probability distribution, we still need to scale it with $ \\left| \\left(\\frac{df(u)}{du}\\right)\\right|^{-1} $.\n",
    "\n",
    "The derivative of $f$ with respect to $u$ is 2, and the inverse is $\\frac{1}{2}$, which is exactly the scaling factor that we need for getting a properly normalized distribution.\n",
    "\n",
    "This, however, is only a simple univariate example. For the multivariate case, instead of the derivative, we need to scale our transformed distribution with the Jacobian determinant:\n",
    "\n",
    "$$ \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{u}} \\right)  \\right|^{-1} = \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}^{-1}}{\\partial \\boldsymbol{x}} \\right)  \\right| $$\n",
    "\n",
    "The main idea of normalizing flows is to learn an invertible mapping between a very complex distribution (for example a distribution of a physics variable) and a very simple distribution. Let us consider the \"generative\" part first, where we attempt to generate new samples from a Normalizing Flow. In this case, we start with a simple distribution, for example, a standard Gaussian, and then repeatedly apply the random variable transformations to acquire a complex distribution similar to the input distribution that we want to approximate. Let's say we apply $k$ transformations $f_{1}...f_{k}$, starting out from a simple distribution $u_{0}$ and going to a complex distribution $u_{k}$, then the change of random variables formula becomes:\n",
    "\n",
    "$$ p(\\boldsymbol{u}_{k}) = p(\\boldsymbol{u}_{0})\\prod_{i} \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{u}_{i-1}} \\right)  \\right|^{-1} $$\n",
    "\n",
    "\n",
    "From this formula, we also see the reason why this method is called \"normalizing flows\". The random variable \"flows\" through a series of transformations while staying normalized by the scaling with the Jacobian determinant.\n",
    "\n",
    "<br />\n",
    "<img src=\"https://drive.google.com/uc?id=1fHZeW9lKGOPl7lZehy8dY0wxLVzjnQFZ\" width=\"1000\">\n",
    "<br />\n",
    "\n",
    "In a practical case, our \"complex\" random variables $\\boldsymbol{u}_{k}$ would then approximate our input variables.\n",
    "\n",
    "Since we choose our $\\boldsymbol{f}_{i}$ to be invertible, we can also run the Normalizing Flow in the other direction, starting out with our complex input distribution and mapping this distribution to a standard Gaussian. This is the direction used for density/likelihood estimation (i.e. if you want to evaluate the likelihood of new data under the learned distribution) and also the one used for training.\n",
    "\n",
    "For training a normalizing flow, remember that our $\\boldsymbol{f}_{i}$ are actually neural networks with parameters $\\theta$. We train our flow by optimizing these parameters in a negative logarithmic likelihood minimization (which is our loss function then). So we take our input samples $\\boldsymbol{u}_{k}$, take it \"backward\" through the flow (i.e. in the direction of the blue arrows in the picture above), and then tune the parameters of our $\\boldsymbol{f}_{i}$ such that the likelihood of the so transformed distribution under a standard Gaussian gets maximized.\n",
    "\n",
    "To be able to use normalizing flows efficiently, we, therefore, need functions $f$ that are both **invertible** and **have a tractable Jacobian** that is **easy to compute**. Several methods have been proposed so far and today, you will work mainly with so-called \"autoregressive flows\", where the functions $f$ are chosen in a way such that the Jacobian is guaranteed to be an upper triangular matrix, so we can compute the determinant simply by multiplying its diagonal elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjLFuT_3Se__"
   },
   "source": [
    "## 2c.) The nFlows package \n",
    "\n",
    "Today, we are going to use the PyTorch nFlows library, which is easy to set up and already contains implementations of many of the different flow models. \n",
    "\n",
    "Details and code of the package can be found here https://github.com/bayesiains/nflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcFRcKKFA4gS"
   },
   "outputs": [],
   "source": [
    "# the nflows functions what we will need in order to build our flow\n",
    "from nflows.flows.base import Flow  # a container that will wrap the parts that make up a normalizing flow\n",
    "from nflows.distributions.normal import StandardNormal  # Gaussian latent space distribution\n",
    "from nflows.transforms.base import (\n",
    "    CompositeTransform,\n",
    ")  # a wrapper to stack simpler transformations to form a more complex one\n",
    "from nflows.transforms.autoregressive import (\n",
    "    MaskedAffineAutoregressiveTransform,\n",
    ")  # the basic transformation, which we will stack several times\n",
    "from nflows.transforms.autoregressive import (\n",
    "    MaskedPiecewiseRationalQuadraticAutoregressiveTransform,\n",
    ")  # the basic transformation, which we will stack several times\n",
    "from nflows.transforms.permutations import ReversePermutation  # a layer that simply reverts the order of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJCJe_sMSfAC"
   },
   "source": [
    "Some background about the different modules:\n",
    "\n",
    "The **flows module** contains the implementation of the actual flow models. The classes in this module already implement the functions to calculate the negative log-likelihood and the jacobian determinant.\n",
    "\n",
    "Every flow model is defined as a series of transformations, contained in the **transformation module**. Here we will be using two different transformations, `MaskedAffineAutoregressiveTransform`, which provides the actual invertible transformation, and `ReversePermutation` which serves to permute the features within the flow. The final transformation `CompositeTransform` allows for the combining of several transformations into one module. \n",
    "\n",
    "Finally, the **distributions module** contains the necessary base distributions that form the latent space. In our case, we will use a gaussian latent space, defined by `StandardNormal`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first model we will want to look at is the Masked Autoregressive Flow (MAF) https://arxiv.org/abs/1705.07057.\n",
    "\n",
    "The autoregressive property on a sequence of data $\\boldsymbol{x} = [x_1, …, x_D]$ states that each output only depends on previous data. This means that the probability of $x_i$ is conditioned only on $x_1, ... ,x_{i-1}$. The full probability density $p(\\boldsymbol{x})$ then becomes the product of the conditional densities:\n",
    "$$p(\\boldsymbol{x}) = \\Pi_{i=1}^{D}p(x_i|x_1, …, x_{i-1})$$\n",
    "\n",
    "An autoregressive flow makes use of this property in arranging the dependencies of the input dimensions. In each layer, the output $f_{i}(\\boldsymbol{x})$ only depends on features $x_1, .. ,x_{i-1}$. The reason for this is the simpler form of the jacobian determinant. The matrix of partial derivates $\\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{x}_{j}}$ becomes triangular because there is no dependency between earlier input dimensions $\\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{x}_{j > i}} = 0$. As a result, the determinant is simply the product of diagonal elements.\n",
    "\n",
    "The variable transformation of the MAF is a shift and scaling operation (i.e. an affine transformation):\n",
    "$$z_i = x_i \\exp(s_i(\\boldsymbol{x})) + t_i(\\boldsymbol{x})$$ \n",
    "but it chooses neural networks $\\boldsymbol{s}$ and $\\boldsymbol{t}$ such that they respect the autoregressive property $s_i(x_1, .., x_{i-1})$ and $t_i(x_1, .., x_{i-1})$. It achieves this by stacking so-called MADE blocks (https://arxiv.org/abs/1502.03509), which are sets of neural network layers that apply element-wise multiplications of mask matrices to the weights. The mask matrices simply contain ones and zeros, intending to turn off dependencies that would violate the autoregressive property. After each flow layer, one applies a permutation on the order of inputs.\n",
    "\n",
    "\n",
    "Let's now define such a simple MAF with the nflows package. In this framework, a flow consists of a base (latent space) distribution and an invertible transformation. This flow object then has two important methods: `flow.log_prob(data)` returns the logarithmic probability of the data, `flow.sample(n_samples)` produces new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5Bx-UG7SfAC"
   },
   "outputs": [],
   "source": [
    "# Fist we define the latent space distribution, in this case a choosing a 2-dim Gaussian\n",
    "base_dist = StandardNormal(shape=[2])\n",
    "\n",
    "# Now we define the series of transformation that our flow model will comprise.\n",
    "# For now we will use a singel MAF layer, we preemtivly implement this as a list,\n",
    "# so to make it simple to extend.\n",
    "\n",
    "# For the MAF layer we need to secify the number of input parameters, as well as the\n",
    "# number of features used in the internal FCN layers.\n",
    "transforms = []\n",
    "transforms.append(MaskedAffineAutoregressiveTransform(features=2, hidden_features=4))\n",
    "\n",
    "# Finally we can combine the list of previously defined transformations into one\n",
    "# single compisite transformation.\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "# The actual flow now consist of the the base distribution and transform together\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "# we can then use standard PyTorch optimizers\n",
    "optimizer = optim.Adam(flow.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J_LURVNjNlF"
   },
   "source": [
    "# 2d.) Data Generation Using Flows\n",
    "\n",
    "One major advantage of Normalizing Flow Networks is that they are inherently invertible. This means that we can not only use them for density estimation by mapping our data set to a well-known distribution like a Normal Gaussian, but we can also do the reverse, mapping the Normal Gaussian to our data distribution. This essentially allows us to generate new data by feeding Gaussian samples backward through our model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly022hdvIC-8"
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i_batch in range(max_batches):\n",
    "    # select the current batch from the dataset\n",
    "    x = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "    x = torch.tensor(x, device=device).float()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # calculate negative log likelihood\n",
    "    nll = -flow.log_prob(x)\n",
    "\n",
    "    # update the model\n",
    "    loss = nll.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwoxoJXQSfAE"
   },
   "outputs": [],
   "source": [
    "# Let's use the fitted distribution and sample from it\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1000).cpu().numpy()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fy5VaGgSfAE"
   },
   "outputs": [],
   "source": [
    "# Let's also plot how well our data is mapped to the Gaussian base distribution\n",
    "\n",
    "inv = flow.transform_to_noise(torch.tensor(X_moons).float().to(device)).detach().cpu().numpy()\n",
    "plt.scatter(inv[:, 0], inv[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlabel(\"Transformed x1\")\n",
    "plt.ylabel(\"Transformed x2\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbtlEySDSfAF"
   },
   "source": [
    "As we can see, our initial density is estimated rather poorly. We also see that our mapped data is far from being Gaussian distributed in the latent space. So what did we do wrong? We actually didn't use a proper flow! For getting to know nFLows, a single MAF transformation was used here. With the knowledge of how to use the module, we can now start building a powerful flow, consisting of multiple layers.\n",
    "\n",
    "**Task: Modify the Code below to improve the generation result**\n",
    "\n",
    "**Bonus Task: Rational Quadratic Splines (RQS) are a more complex and advanced type of invertible transformations, that see use in state-of-the-art normalizing flows. nFlows provides a prebuilt implementation of these transformations, which can be accessed via:**\n",
    "\n",
    "```\n",
    "MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=2, \n",
    "hidden_features=16, tail_bound = 3.0, tails = \"linear\"))\n",
    "```\n",
    "\n",
    "**Try replacing the MAF transformations with RQS transformations and see what effects can be observed observe.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odmPNp-hSfAF"
   },
   "source": [
    "# 2e.) Two Moons Generation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K23FrzYXSfAF"
   },
   "outputs": [],
   "source": [
    "base_dist = StandardNormal(shape=[2])\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "transforms = []\n",
    "transforms.append(MaskedAffineAutoregressiveTransform(features=2, hidden_features=16))\n",
    "transforms.append(ReversePermutation(features=2))\n",
    "######\n",
    "\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters())\n",
    "\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "num_epochs = 1\n",
    "batch_size = 200\n",
    "######\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    permut = np.random.permutation(X_moons.shape[0])\n",
    "    X_moons_shuffle = X_moons[permut]\n",
    "    for i_batch in range(max_batches):\n",
    "        x = X_moons_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nll = -flow.log_prob(x)\n",
    "\n",
    "        loss = nll.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('Epoch: {:d}'.format(i))\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1000).cpu().numpy()\n",
    "\n",
    "# transform them like we did when plotting the original dataset\n",
    "# samples = StandardScaler().fit_transform(samples)\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvmCqnb-LJys"
   },
   "source": [
    "# 2f.) Intermediate transformation steps\n",
    "\n",
    "So far, our implementation of normalizing flows was very effective and easy to use, however it was also rather BlackBox-y. \n",
    "\n",
    "The code below illustrates how one can extract the individual transformations and the base distribution from the flow model, and then perform the first `n_transforms` transformations \"by hand\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFAJO-nNLUJQ"
   },
   "outputs": [],
   "source": [
    "# Number of transforms to perform\n",
    "n_transforms = 1\n",
    "\n",
    "# Ensure the number of transforms to perform is lower than the total amount of transforms\n",
    "n_transforms = min(len(flow._transform._transforms), n_transforms)\n",
    "\n",
    "# Here we access the base distribution of the flow model and sample from it\n",
    "z = flow._distribution.sample(1000)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, n_transforms):\n",
    "        # The flow model has an attribute called '_transform', which again has an attribute\n",
    "        # '_transforms', which is a list containing the individual transforms\n",
    "        # For the purpose of generating samples we need to iterate through the transforms in\n",
    "        # reverse order, hence the negative index\n",
    "        m = flow._transform._transforms[-i]\n",
    "\n",
    "        # The transforms can be applied like any other PyTorch module, however, we need\n",
    "        # to specify the direction, in the case of sampling we need the inverse.\n",
    "        # The function has two return parameters, the transformed data, and the log-determinant\n",
    "        # for the sample generation we only care about the former and map the latter to a _\n",
    "        z, _ = m.inverse(z)\n",
    "\n",
    "\n",
    "plt.scatter(z.cpu().numpy()[:, 0], z.cpu().numpy()[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsaNorE-UKoG"
   },
   "source": [
    "So far we only see the effects of the first transformation\n",
    "\n",
    "**Task: Use the above code to plot the intermediate distributions for all flow transformations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhipO0QxQHU6"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Add code here #\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOkCS4rTZBKK"
   },
   "source": [
    "## 2g.) Conditional Generative Flows\n",
    "\n",
    "Many generative tasks in particle physics require the generative model to not only generate a random point from the data set but instead generate a point with a specific property. One example is shower simulation, where a generative shower simulator should be able to produce showers for a given particle energy, rather than just for a random one. \n",
    "\n",
    "This is where conditional generative models become important. In the previous example, the energy value would be the condition given to the model. \n",
    "\n",
    "Let us try this principle on flows with our previous two-moons data set. As one can see, it is made up of two distinct subsets, (the eponymous moons). Our conditioning will be to which moon a datapoint belongs, e.g. if we tell the model to generate points with the label 0 they should be from the top moon, while points with the label 1 should be from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BHN2hoKaDNC"
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for two moons dataset\n",
    "n = 10000\n",
    "# the two moons sampling function already returns labels to which moon a data point belongs\n",
    "X_moons, label_moon = datasets.make_moons(n_samples=n, noise=0.05)\n",
    "X_moons, mask = normalize_moons(X_moons)\n",
    "label_moon = label_moon[mask]\n",
    "\n",
    "\n",
    "plt.scatter(X_moons[:, 0][label_moon == 0], X_moons[:, 1][label_moon == 0], color='orangered', marker='.', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0][label_moon == 1], X_moons[:, 1][label_moon == 1], color='green', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./original_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXqZ5KrtzVqM"
   },
   "source": [
    "### Conditional nFlows models\n",
    "\n",
    "Conditional models in nFlows are handled via the `context` arguments. During the definition of each transform layer, the number of conditional variables has to be specified. \n",
    "\n",
    "Then, during training and generation, the context has to be passed as additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5rYwUtonxFc"
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_layers = 10\n",
    "base_dist = StandardNormal(shape=[n_features])\n",
    "\n",
    "transforms = []\n",
    "for i in range(0, n_layers):\n",
    "    transforms.append(\n",
    "        MaskedAffineAutoregressiveTransform(features=n_features, hidden_features=16, context_features=1)\n",
    "    )  # note the context_features argument\n",
    "    transforms.append(ReversePermutation(features=n_features))\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters())\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    permut = np.random.permutation(X_moons.shape[0])\n",
    "    X_moons_shuffle = X_moons[permut]\n",
    "    label_moon_shuffle = label_moon[permut]\n",
    "\n",
    "    for i_batch in range(max_batches):\n",
    "        x = X_moons_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        y = label_moon_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        y = torch.tensor(y, device=device).float().view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nll = -flow.log_prob(x, context=y)  # note the context argument\n",
    "\n",
    "        loss = nll.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('Epoch: {:d}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_QgmwT5zGI9"
   },
   "source": [
    "Now lets see how well our model performs, first by using random labels. This should look like the default two moons set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNcs7DP-p4ON"
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "cond = np.random.randint(2, size=(n_samples, 1))\n",
    "cond = torch.tensor(cond).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1, context=cond).view(n_samples, -1).cpu().numpy()\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO8tZDAYbYj3"
   },
   "source": [
    "We can also test how well our conditioning performs by passing only the label 0 or only label 1 to our network. \n",
    "\n",
    "**Task: Complete the code below to generate points from the two moons separately, and then plot the two moons in separate colors**\n",
    "\n",
    "**Bonus task: What happens when the flow is evaluated using labels different from 0 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1mTh2tCroPU"
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "samples1 = ...\n",
    "samples0 = ...\n",
    "\n",
    "plt.scatter(samples0[:, 0], samples0[:, 1], color='orangered', marker='.', linewidth=0)\n",
    "plt.scatter(samples1[:, 0], samples1[:, 1], color='green', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ALnN9Vewh3Y"
   },
   "source": [
    "# 3.) High energy physics use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49n56ljzwqAi"
   },
   "source": [
    "To illustrate the use of normalizing flows, we now want to apply it to a high energy physics example. For this aim, we use the LHCO 2020 challenge R&D dataset (https://lhco2020.github.io/homepage/). It consists of simulated QCD multijet events in addition to a signal process of a W' boson with a mass of $m_{W'} = 3.5$ TeV decaying into an X boson ($m_{X} = 500$ GeV) and a Y boson ($m_{Y} = 100$ GeV). Specifically, we use the high-level features set, which contains the 4-momenta of the two leading jets, their (1,2,3) subjettiness, and a label denoting whether the event is a signal (1) or background (0) process.\n",
    "\n",
    "While the main purpose of this dataset was an anomaly detection competition, we will continue looking at normalizing flows from the perspective of artificial data generation. Nevertheless, if you manage to solve the advanced exercise, in the end, you will also be well prepared to understand and use the ANODE method (Anomaly Detection with Density Estimation, <a href=\"https://arxiv.org/abs/2001.04990\">arxiv</a>).\n",
    "\n",
    "So let's first download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltEQiOQtwvWR"
   },
   "source": [
    "The data is stored as a pandas dataframe. We load the data and pick a random 10% subset for the sake of computation speed. Feel free to use the full data in your own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E6vLfBywxPH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_hdf(\"/global/cfs/cdirs/ntrain2/generative/events_anomalydetection_v2.features.h5\")\n",
    "dataset = dataset.sample(frac=0.1)\n",
    "print(\"The variables are:\", [col for col in dataset.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5so7o1UBw0of"
   },
   "source": [
    "We now order the two jets by mass, such that $m_{j1} \\leq m_{j2}$. Then we extract four interesting features, which could be used here to distinguish signal from background: $m_{j1}$, $m_{j2}-m_{j1}$, $\\tau_{21,1}$, $\\tau_{21,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlJYsGU8w2Wm"
   },
   "outputs": [],
   "source": [
    "mj1mj2 = np.array(dataset[['mj1', 'mj2']])\n",
    "tau21 = np.array(dataset[['tau2j1', 'tau2j2']]) / (\n",
    "    1e-5 + np.array(dataset[['tau1j1', 'tau1j2']])\n",
    ")  # add small number to denominator to avoid division by zero\n",
    "mjmin = mj1mj2[range(len(mj1mj2)), np.argmin(mj1mj2, axis=1)]\n",
    "mjmax = mj1mj2[range(len(mj1mj2)), np.argmax(mj1mj2, axis=1)]\n",
    "tau21min = tau21[range(len(mj1mj2)), np.argmin(mj1mj2, axis=1)]\n",
    "tau21max = tau21[range(len(mj1mj2)), np.argmax(mj1mj2, axis=1)]\n",
    "\n",
    "\n",
    "data = np.dstack((mjmin / 1000, (mjmax - mjmin) / 1000, tau21min, tau21max))[0]  # put data together and convert GeV to TeV\n",
    "labels = np.array(dataset['label'])\n",
    "print(\"data.shape =\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UJ8WEKvw5Hu"
   },
   "source": [
    "Let's plot these four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9xbcZsXw65-"
   },
   "outputs": [],
   "source": [
    "## visualize the 4 extracted variables\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "_, common_bins, _ = ax[0, 0].hist(data[:, 0], 100, alpha=0.6, label=\"data\")\n",
    "ax[0, 0].hist(data[:, 0][labels == 0], common_bins, label=\"background\", edgecolor=\"red\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 0].hist(data[:, 0][labels == 1], common_bins, label=\"signal\", edgecolor=\"green\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 0].set_xlabel(\"lower jet mass (TeV)\")\n",
    "ax[0, 0].set_ylabel(\"events\")\n",
    "ax[0, 0].set_yscale(\"log\")\n",
    "_, common_bins, _ = ax[0, 1].hist(data[:, 1], 100, alpha=0.6, label=\"data\")\n",
    "ax[0, 1].hist(data[:, 1][labels == 0], common_bins, label=\"background\", edgecolor=\"red\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 1].hist(data[:, 1][labels == 1], common_bins, label=\"signal\", edgecolor=\"green\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 1].set_xlabel(\"jet mass difference (TeV)\")\n",
    "ax[0, 1].set_ylabel(\"events\")\n",
    "ax[0, 1].set_yscale(\"log\")\n",
    "_, common_bins, _ = ax[1, 0].hist(data[:, 2], 100, alpha=0.6, label=\"data\")\n",
    "ax[1, 0].hist(data[:, 2][labels == 0], common_bins, label=\"background\", edgecolor=\"red\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 0].hist(data[:, 2][labels == 1], common_bins, label=\"signal\", edgecolor=\"green\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 0].set_xlabel(r\"$\\tau_{21,1}$\")\n",
    "ax[1, 0].set_ylabel(\"events\")\n",
    "_, common_bins, _ = ax[1, 1].hist(data[:, 3], 100, alpha=0.6, label=\"data\")\n",
    "ax[1, 1].hist(data[:, 3][labels == 0], common_bins, label=\"background\", edgecolor=\"red\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 1].hist(data[:, 3][labels == 1], common_bins, label=\"signal\", edgecolor=\"green\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 1].set_xlabel(r\"$\\tau_{21,2}$\")\n",
    "ax[1, 1].set_ylabel(\"events\")\n",
    "leg_handles, leg_labels = ax[0, 0].get_legend_handles_labels()\n",
    "fig.legend(leg_handles, leg_labels, loc='upper right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N0H01gTw9t2"
   },
   "source": [
    "We want train a density estimator to generate such data ourselves. We use the same MAF model as before, just reinstantiate it. This time with 4 input variables. In addition, we increase the complexity of the MADE layers in order to keep up with the more complex task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAUKIc2Tw_8f"
   },
   "outputs": [],
   "source": [
    "n_features = 4\n",
    "n_layers = 6\n",
    "base_dist = StandardNormal(shape=[n_features])\n",
    "\n",
    "transforms = []\n",
    "for i in range(0, n_layers):\n",
    "    transforms.append(MaskedAffineAutoregressiveTransform(features=n_features, hidden_features=128, context_features=None))\n",
    "    transforms.append(ReversePermutation(features=n_features))\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRuMFBwDxBq7"
   },
   "source": [
    "Before plugging our data into the MAF, we should preprocess the variables, so the model can learn them more easily. Density estimators tend to have difficulties with sharp edges and boundaries. We first normalize each variable to lie between 0 and 1, then we apply a logit transformation $\\log(\\frac{x}{1-x})$ mapping the variables to ($-\\infty$, $+\\infty$), and then we shift the mean to 0 and divide by the standard deviation. Here we define these three transformations, as well as their inverse functions. Be aware that for computing likelihoods, we would also need to take into account the Jacobian of each transformation. In a purely generative approach, we don't need to worry about this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1gwGkwkxDs2"
   },
   "outputs": [],
   "source": [
    "## defining variable transformations\n",
    "\n",
    "\n",
    "def normalize_data(in_data, max_val, min_val):\n",
    "    new_data = (in_data - min_val) / (max_val - min_val)\n",
    "    mask = np.prod(((new_data < 1) & (new_data > 0)), axis=1, dtype=bool)\n",
    "    new_data = new_data[mask]\n",
    "    return new_data, mask\n",
    "\n",
    "\n",
    "def logit_data(in_data):\n",
    "    new_data = np.log(in_data / (1 - in_data))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def standardize_data(in_data, mean_val, std_val):\n",
    "    new_data = (in_data - mean_val) / std_val\n",
    "    return new_data\n",
    "\n",
    "\n",
    "## defining their inverse transformations\n",
    "\n",
    "\n",
    "def normalize_inverse(in_data, max_val, min_val):\n",
    "    new_data = in_data * (max_val - min_val) + min_val\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def logit_inverse(in_data):\n",
    "    new_data = (1 + np.exp(-in_data)) ** (-1)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def standardize_inverse(in_data, mean_val, std_val):\n",
    "    new_data = std_val * in_data + mean_val\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-STT0vaxGf9"
   },
   "outputs": [],
   "source": [
    "## transform data and save max, min, mean, std values for the backtransformation later\n",
    "\n",
    "max_values = np.max(data, keepdims=True, axis=0)\n",
    "min_values = np.min(data, keepdims=True, axis=0)\n",
    "\n",
    "## normalize\n",
    "transformed_data, mask = normalize_data(data, max_values, min_values)\n",
    "\n",
    "## logit\n",
    "transformed_data = logit_data(transformed_data)\n",
    "\n",
    "## standardize\n",
    "mean_values = np.mean(transformed_data, keepdims=True, axis=0)\n",
    "std_values = np.std(transformed_data, keepdims=True, axis=0)\n",
    "transformed_data = standardize_data(transformed_data, mean_values, std_values)\n",
    "\n",
    "## apply mask also to labels\n",
    "transformed_labels = labels[mask]\n",
    "transformed_labels = np.expand_dims(transformed_labels, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88V4XWXrxIsr"
   },
   "source": [
    "Now we are ready to train the model again. Here we just train for 10 epochs (maybe even decrease it to 5 if you couldn't get a GPU). The resulting density estimate might not be perfectly optimized, but it will illustrate its power on a HEP example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZTm4MOxxKxd"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "max_batches = int(transformed_data.shape[0] / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    permut = np.random.permutation(transformed_data.shape[0])\n",
    "    transformed_data_shuffle = transformed_data[permut]\n",
    "\n",
    "    for i_batch in range(max_batches):\n",
    "        x = transformed_data_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nll = -flow.log_prob(x)\n",
    "\n",
    "        loss = nll.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print('Epoch: {:d}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2UeTA_ExNK4"
   },
   "source": [
    "Then we sample artificial data points from this learned density. We produce the same number of datapoints that we have in our training data. Then we have to perform all the variable transformations backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRwgPbxmxOp-"
   },
   "outputs": [],
   "source": [
    "## sample artificial data points\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(data.shape[0]).cpu().numpy()\n",
    "\n",
    "## inverse standardize\n",
    "retransformed_samples = standardize_inverse(samples, mean_values, std_values)\n",
    "\n",
    "## inverse logit\n",
    "retransformed_samples = logit_inverse(retransformed_samples)\n",
    "\n",
    "## inverse normalize\n",
    "retransformed_samples = normalize_inverse(retransformed_samples, max_values, min_values)\n",
    "\n",
    "print(\"sampled data shape =\", retransformed_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrwBpUoixRvf"
   },
   "source": [
    "Let's plot the original data again and in addition the sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV5cCEkKxUxu"
   },
   "outputs": [],
   "source": [
    "## plot the sampled against the original data\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "_, common_bins, _ = ax[0, 0].hist(data[:, 0], 100, alpha=0.6, label=\"data\")\n",
    "ax[0, 0].hist(retransformed_samples[:, 0], common_bins, label=\"sampled\", edgecolor=\"orange\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 0].set_xlabel(\"lower jet mass (TeV)\")\n",
    "ax[0, 0].set_ylabel(\"events\")\n",
    "ax[0, 0].set_yscale(\"log\")\n",
    "_, common_bins, _ = ax[0, 1].hist(data[:, 1], 100, alpha=0.6, label=\"data\")\n",
    "ax[0, 1].hist(retransformed_samples[:, 1], common_bins, label=\"sampled\", edgecolor=\"orange\", linewidth=1.3, histtype=\"step\")\n",
    "ax[0, 1].set_xlabel(\"jet mass difference (TeV)\")\n",
    "ax[0, 1].set_ylabel(\"events\")\n",
    "ax[0, 1].set_yscale(\"log\")\n",
    "_, common_bins, _ = ax[1, 0].hist(data[:, 2], 100, alpha=0.6, label=\"data\")\n",
    "ax[1, 0].hist(retransformed_samples[:, 2], common_bins, label=\"sampled\", edgecolor=\"orange\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 0].set_xlabel(r\"$\\tau_{21,1}$\")\n",
    "ax[1, 0].set_ylabel(\"events\")\n",
    "_, common_bins, _ = ax[1, 1].hist(data[:, 3], 100, alpha=0.6, label=\"data\")\n",
    "ax[1, 1].hist(retransformed_samples[:, 3], common_bins, label=\"sampled\", edgecolor=\"orange\", linewidth=1.3, histtype=\"step\")\n",
    "ax[1, 1].set_xlabel(r\"$\\tau_{21,2}$\")\n",
    "ax[1, 1].set_ylabel(\"events\")\n",
    "leg_handles, leg_labels = ax[0, 0].get_legend_handles_labels()\n",
    "fig.legend(leg_handles, leg_labels, loc='upper right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vpx1fq0Qa46r"
   },
   "source": [
    "\n",
    "\n",
    "Even though we haven't fully trained the model, there is a striking agreement already between the original and the sampled distribution. We want to make sure now that the model has also learned the correct correlations between the four features. Let's compute the Pearson correlation coefficients and compare the two correlation matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8WPgelExadO"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "variable_labels = [r'$m_{j1}$', r'$m_{j2}-m_{j1}$', r'$\\tau_{21,1}$', r'$\\tau_{21,2}$']\n",
    "corr_matrix_data = np.corrcoef(np.transpose(data))\n",
    "\n",
    "corr_matrix_data = pd.DataFrame(np.corrcoef(np.transpose(data)), columns=variable_labels, index=variable_labels)\n",
    "corr_matrix_sample = pd.DataFrame(\n",
    "    np.corrcoef(np.transpose(retransformed_samples)), columns=variable_labels, index=variable_labels\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(corr_matrix_data, vmax=1.0, vmin=-1.0, cmap='coolwarm', annot=True, square=True)\n",
    "plt.title(\"data correlations\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(corr_matrix_sample, vmax=1.0, vmin=-1.0, cmap='coolwarm', annot=True, square=True)\n",
    "plt.title(\"sample correlations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1Q4bKNt_SY_"
   },
   "source": [
    "While the comparison might still yield some room for improvement, it's clear that the normalizing flow has indeed learned the correlations between the features, rather than just sampling four independent distributions.\n",
    "\n",
    "**Task: Use your knowledge from the previous section to train a conditional flow that can separately generate signal and background events. Sample from these separately and plot them in comparison to the actual data as before. This works best if you sample the same number of signal and background events as present in the data respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHYGXMqx2tua"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Add code here\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFe2BWVBTAPn"
   },
   "source": [
    "## 3b.) Advanced Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKF0ZBzvQnJW"
   },
   "source": [
    "If the exercises before were too easy for you or if you would like to have an additional challenge in your own time, consider the following task. It's not rocket science but it's very likely to take more time than what we have in this exercise class.\n",
    "\n",
    "So far, we have for the sake of simplicity only used a binary conditional variable. However, this would work just as well on a general continuous conditional. If you extracted also the dijet mass (mjj) from the dataset and plotted it. You would see that the signal peaks quite sharply at the resonance mass of 3.5 TeV. We can divide the mjj spectrum into a signal region (SR), ranging from 3.3 to 3.7 TeV, and the remaining sideband region (SB). Drawing events from the SB, you should only see very background-like distributions in the other variables, whereas the SR events should yield a very distinct signal peak in addition to the background. \n",
    "\n",
    "**Task: Extract and plot mjj. Train a conditional flow model with mjj as the conditional variable, then sample data from the two regions separately in the same ratio as found in the data. Plot the variables and compare them with the original data. Since this task is also more difficult for the density estimator, you might need to increase the model complexity, the number of training epochs and/or the fraction of used data to achieve a satisfying result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKCBbMZSUK8N"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Add code here\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbNI8qfkGSm8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "159Uova_QyCMPi8ar-y-V2ouzWSpztUK1",
     "timestamp": 1669211085641
    },
    {
     "file_id": "1tfeaFYKTmHLOnICVp_Aw_EFSpiqmQ_Cg",
     "timestamp": 1669133606638
    },
    {
     "file_id": "1iBnBglMcvm4ajE6tde6OnUfxHvqHeWYq",
     "timestamp": 1668762589979
    },
    {
     "file_id": "1OGUkPIhnEgDFUgoiSb8UlbrNi0WiSUSy",
     "timestamp": 1615198767494
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ATLAS Training (PyTorch)",
   "language": "python",
   "name": "training_env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
